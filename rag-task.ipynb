{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f8de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:57:53.849195Z",
     "iopub.status.busy": "2025-11-12T14:57:53.848609Z",
     "iopub.status.idle": "2025-11-12T14:58:26.787936Z",
     "shell.execute_reply": "2025-11-12T14:58:26.787104Z",
     "shell.execute_reply.started": "2025-11-12T14:57:53.849163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys, platform, random, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import langchain\n",
    "import langgraph\n",
    "import sentence_transformers\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers transformers accelerate pypdf datasets ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a27bf",
   "metadata": {},
   "source": [
    "### Beállítások (Config)\n",
    "\n",
    "Adatforrások, modellek, RAG-paraméterek és hiperparaméterek a reprodukálhatóság érdekében."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b427ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:58:26.789329Z",
     "iopub.status.busy": "2025-11-12T14:58:26.788732Z",
     "iopub.status.idle": "2025-11-12T14:58:26.794296Z",
     "shell.execute_reply": "2025-11-12T14:58:26.793453Z",
     "shell.execute_reply.started": "2025-11-12T14:58:26.789308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "USE_RAG = True\n",
    "\n",
    "PDF_URLS = [\"https://arxiv.org/pdf/1706.03762.pdf\", \n",
    "\"https://www.medrxiv.org/content/10.1101/2024.08.10.24311686v1.full.pdf\",\n",
    "\"https://arxiv.org/pdf/2107.09559\",]\n",
    "PDF_PATHS = [\"./data/attention_is_all_you_need.pdf\",\n",
    "\"./data/brainagenext.pdf\",\n",
    "\"./data/synthseg.pdf\",]\n",
    "\n",
    "#MODELS\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/e5-small-v2\"                 \n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" \n",
    "LLM_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\" # \"microsoft/phi-1_5\"   \n",
    "\n",
    "#RAG\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 8                \n",
    "RERANK_TOP_K = 5          \n",
    "ENABLE_RERANKER = True   \n",
    "\n",
    "#HYPERPARAMS\n",
    "MAX_STEPS_PER_SUBTASK = 2  # num of retries\n",
    "MIN_REFLECT_SCORE = 0.6    # retry decision\n",
    "MAX_NEW_TOKENS = 300\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1718ed",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "PDF-ek letöltése, beolvasása (PyPDFLoader) és szeletelése (RecursiveCharacterTextSplitter). `chunk_id` metaadat a későbbi citációkhoz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193611b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:58:26.796302Z",
     "iopub.status.busy": "2025-11-12T14:58:26.796097Z",
     "iopub.status.idle": "2025-11-12T14:58:38.194499Z",
     "shell.execute_reply": "2025-11-12T14:58:38.193731Z",
     "shell.execute_reply.started": "2025-11-12T14:58:26.796285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./data/attention_is_all_you_need.pdf PDF...\n",
      "Saved to data/attention_is_all_you_need.pdf\n",
      "Downloading ./data/brainagenext.pdf PDF...\n",
      "Saved to data/brainagenext.pdf\n",
      "Downloading ./data/synthseg.pdf PDF...\n",
      "Saved to data/synthseg.pdf\n",
      "Downloaded 3 scientific papers\n",
      "Loaded 58 raw documents\n",
      "Chunked into 305 chunks\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(len(PDF_URLS)):\n",
    "        out_path = Path(PDF_PATHS[i])\n",
    "        if not out_path.exists():\n",
    "            print(f\"Downloading {PDF_PATHS[i]} PDF...\")\n",
    "            urllib.request.urlretrieve(PDF_URLS[i], str(out_path))\n",
    "            print(\"Saved to\", out_path)\n",
    "    print(f\"Downloaded {len(PDF_PATHS)} scientific papers\")\n",
    "\n",
    "\n",
    "def load_documents() -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in PDF_PATHS:\n",
    "        loader = PyPDFLoader(p)\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    print(f\"Loaded {len(docs)} raw documents\")\n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    splits = splitter.split_documents(docs)\n",
    "    # chunk_id to CITE\n",
    "    for i, d in enumerate(splits):\n",
    "        d.metadata = dict(d.metadata)\n",
    "        d.metadata[\"chunk_id\"] = i\n",
    "    print(f\"Chunked into {len(splits)} chunks\")\n",
    "    return splits\n",
    "\n",
    "download_data()\n",
    "raw_docs = load_documents()\n",
    "chunks = chunk_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb617d0",
   "metadata": {},
   "source": [
    "### Embedding and Vector database\n",
    "\n",
    "E5 beágyazások + FAISS index; a retriever `k=TOP_K` paraméterrel dolgozik a hasonlósági kereséshez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650f2ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:58:38.195828Z",
     "iopub.status.busy": "2025-11-12T14:58:38.195168Z",
     "iopub.status.idle": "2025-11-12T14:58:48.143327Z",
     "shell.execute_reply": "2025-11-12T14:58:48.142514Z",
     "shell.execute_reply.started": "2025-11-12T14:58:38.195806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "class E5Embeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode([f\"passage: {t}\" for t in texts], normalize_embeddings=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode([f\"query: {text}\"], normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "embeddings = E5Embeddings(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Vector store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecef1c",
   "metadata": {},
   "source": [
    "### Rerank\n",
    "\n",
    "CrossEncoder (ms-marco-MiniLM-L-6-v2) a top találatok újrapontozására a esetleges minőség javításra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0397d86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:58:48.144540Z",
     "iopub.status.busy": "2025-11-12T14:58:48.144192Z",
     "iopub.status.idle": "2025-11-12T14:58:50.472845Z",
     "shell.execute_reply": "2025-11-12T14:58:50.472083Z",
     "shell.execute_reply.started": "2025-11-12T14:58:48.144514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(RERANKER_MODEL_NAME) if ENABLE_RERANKER else None\n",
    "\n",
    "def rerank(query: str, docs: List[Document], top_k: int = RERANK_TOP_K) -> List[Document]:\n",
    "\n",
    "    pairs = [(query, d.page_content[:512]) for d in docs] \n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    idx = np.argsort(scores)[::-1]  \n",
    "    return [docs[i] for i in idx[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e514223",
   "metadata": {},
   "source": [
    "### LLM Model Init\n",
    "Transformers pipeline helyi, cserélhető modellhez; `max_new_tokens` és `temperature` a válasz stílusát/hosszát szabályozza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fe914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:58:50.474050Z",
     "iopub.status.busy": "2025-11-12T14:58:50.473820Z",
     "iopub.status.idle": "2025-11-12T14:59:14.847945Z",
     "shell.execute_reply": "2025-11-12T14:59:14.847175Z",
     "shell.execute_reply.started": "2025-11-12T14:58:50.474032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "def build_llm(model_name: str, max_new_tokens: int = MAX_NEW_TOKENS, temperature: float = TEMPERATURE):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")  \n",
    "        gen = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=max_new_tokens, do_sample=(temperature>0), temperature=temperature)\n",
    "        def llm(prompt: str) -> str:\n",
    "            out = gen(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "            return out.strip()  \n",
    "        print(f\"Loaded LLM: {model_name}\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load LLM. Error:\", e)\n",
    "\n",
    "llm = build_llm(LLM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2ff33",
   "metadata": {},
   "source": [
    "### Agent Node Components\n",
    "- `plan`: a felhasználói kérdést ≤4 részfeladatra bontja.\n",
    "- `retrieve`: FAISS-ból kontextus visszakeresése (majd opcionális rerank).\n",
    "- `synthesize`: LLM válasz a kontextusból, kötelező inline citációkkal [i].\n",
    "- `reflect`: heurisztikus minőségellenőrzés (hossz + citációk).\n",
    "- `finalize`: részválaszok koherens összevonása + „Sources” szekció."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e03dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:59:14.849472Z",
     "iopub.status.busy": "2025-11-12T14:59:14.848833Z",
     "iopub.status.idle": "2025-11-12T14:59:14.909202Z",
     "shell.execute_reply": "2025-11-12T14:59:14.908586Z",
     "shell.execute_reply.started": "2025-11-12T14:59:14.849451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: list[str]                   \n",
    "    step_idx: int                     \n",
    "    retrieved: list[Document]        \n",
    "    partial_answers: list[str]       \n",
    "    citations: list[list[Dict[str, Any]]] \n",
    "    final_answer: Optional[str]\n",
    "    reflect_score: float\n",
    "    history: list[Dict[str, Any]]     \n",
    "\n",
    "\n",
    "def plan_node(state: AgentState) -> AgentState:\n",
    "    q = state[\"question\"]\n",
    "    prompt = f\"\"\"You are a planning agent. Split the user's question into up to 4 logical subtasks as a JSON list.\n",
    "\n",
    "    Question: {q}\n",
    "\n",
    "    Output example: [\"Define the concept\", \"Explain the background\", \"Give practical examples\"]\"\"\"\n",
    "    try:\n",
    "        out = llm(prompt)\n",
    "        plan = json.loads(out) if out.strip().startswith(\"[\") else [q]\n",
    "    except Exception:\n",
    "        plan = [q]\n",
    "    state[\"plan\"] = plan[:4] if plan else [q]\n",
    "    state[\"step_idx\"] = 0\n",
    "    state[\"partial_answers\"] = []\n",
    "    state[\"citations\"] = []\n",
    "    state[\"history\"] = state.get(\"history\", []) + [{\"role\": \"system\", \"content\": f\"Plan: {state['plan']}\"}]\n",
    "    return state\n",
    "\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    step = state[\"plan\"][state[\"step_idx\"]]\n",
    "    docs = retriever.invoke(step)\n",
    "    docs = rerank(step, docs, top_k=RERANK_TOP_K) if ENABLE_RERANKER else docs[:RERANK_TOP_K]\n",
    "    state[\"retrieved\"] = docs\n",
    "    state[\"history\"].append({\"role\": \"tool\", \"content\": f\"Retrieved {len(docs)} docs for step '{step}'\"})\n",
    "    return state\n",
    "\n",
    "\n",
    "def synthesize_node(state: AgentState) -> AgentState:\n",
    "    step = state[\"plan\"][state[\"step_idx\"]]\n",
    "    ctx_docs = state[\"retrieved\"][:4]\n",
    "    context = \"\\n\\n\".join([f\"[{i}] {d.page_content[:1200]}\" for i, d in enumerate(ctx_docs)])\n",
    "    \n",
    "    prompt = f\"\"\"You are answering a scientific question using provided sources.\n",
    "\n",
    "    Subtask: {step}\n",
    "\n",
    "    Sources:\n",
    "    {context}\n",
    "\n",
    "    Instructions:\n",
    "    1. Write a clear, direct answer to the subtask\n",
    "    2. Include inline citations [0], [1], [2] next to every claim\n",
    "    3. Do NOT just list the sources - write actual explanatory text\n",
    "    4. Keep it concise but informative\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    answer = llm(prompt)\n",
    "    cited_ids = sorted(set(int(x) for x in re.findall(r\"\\[(\\d+)\\]\", answer) if x.isdigit() and int(x) < len(ctx_docs)))\n",
    "    citations = [\n",
    "        {\n",
    "            \"chunk_id\": ctx_docs[i].metadata.get(\"chunk_id\"),\n",
    "            \"metadata\": ctx_docs[i].metadata\n",
    "        }\n",
    "        for i in cited_ids]\n",
    "    state[\"partial_answers\"].append(answer)\n",
    "    state[\"citations\"].append(citations)\n",
    "    state[\"history\"].append({\"role\": \"assistant\", \"content\": f\"Draft answer for '{step}'\"})\n",
    "    return state\n",
    "\n",
    "def reflect_node(state: AgentState) -> AgentState:\n",
    "    ans = state[\"partial_answers\"][-1] if state[\"partial_answers\"] else \"\"\n",
    "    has_cite = \"[\" in ans and \"]\" in ans\n",
    "    length_ok = len(ans.split()) > 25\n",
    "    score = 0.5 * (1.0 if has_cite else 0.0) + 0.5 * (1.0 if length_ok else 0.0)\n",
    "    state[\"reflect_score\"] = float(score)\n",
    "    state[\"history\"].append({\"role\": \"system\", \"content\": f\"Reflect score: {score:.2f}\"})\n",
    "    return state\n",
    "\n",
    "def finalize_node(state: AgentState) -> AgentState:\n",
    "    combined = \"\\n\\n\".join([f\"Subtask {i+1}: {p}\\nAnswer:\\n{a}\" for i, (p, a) in enumerate(zip(state[\"plan\"], state[\"partial_answers\"]))])\n",
    "    prompt = f\"\"\"Merge the sub-answers into a coherent final answer. Preserve all inline citations like [0], [1].\n",
    "        \n",
    "    Sub-answers:\n",
    "    {combined}\n",
    "\n",
    "    Write a clear, well-structured final answer that preserves all citations:\"\"\"\n",
    "        \n",
    "    final = llm(prompt)\n",
    "    state[\"final_answer\"] = final\n",
    "    \n",
    "    all_citations = {}\n",
    "    for subtask_cites in state[\"citations\"]:\n",
    "        for cite in subtask_cites:\n",
    "            chunk_id = cite[\"chunk_id\"]\n",
    "            if chunk_id is not None and chunk_id not in all_citations:\n",
    "                all_citations[chunk_id] = cite[\"metadata\"]\n",
    "    \n",
    "    if all_citations:\n",
    "        citation_text = \"\\n\\nSources:\\n\"\n",
    "        for i, (chunk_id, metadata) in enumerate(all_citations.items()):\n",
    "            source = metadata.get('source', 'Unknown')\n",
    "            page = metadata.get('page', 'N/A')\n",
    "            citation_text += f\"[{i}] {source}, page {page}\\n\"\n",
    "        state[\"final_answer\"] = final + citation_text\n",
    "    \n",
    "    state[\"history\"].append({\"role\": \"assistant\", \"content\": \"Final answer synthesized.\"})\n",
    "    return state\n",
    "\n",
    "def should_retry(state: AgentState) -> bool:\n",
    "    # Retry if quality is low and we haven't exhausted attempts\n",
    "    attempts_this_step = sum(1 for h in state[\"history\"] if h.get(\"role\")==\"assistant\" and \"Draft answer\" in h.get(\"content\",\"\"))\n",
    "    return (state[\"reflect_score\"] < MIN_REFLECT_SCORE) and (attempts_this_step < MAX_STEPS_PER_SUBTASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aefe437",
   "metadata": {},
   "source": [
    "### LangGraph Architecture\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  plan --> retrieve --> synthesize --> reflect\n",
    "  reflect -- retry (loop back) --> retrieve\n",
    "  reflect -- next subtask (loop back) --> retrieve\n",
    "  reflect -- finalize --> finalize --> OUT((END))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1605d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:59:14.910085Z",
     "iopub.status.busy": "2025-11-12T14:59:14.909874Z",
     "iopub.status.idle": "2025-11-12T14:59:17.193900Z",
     "shell.execute_reply": "2025-11-12T14:59:17.193114Z",
     "shell.execute_reply.started": "2025-11-12T14:59:14.910069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"plan\", plan_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"synthesize\", synthesize_node)\n",
    "graph.add_node(\"reflect\", reflect_node)\n",
    "graph.add_node(\"finalize\", finalize_node)\n",
    "\n",
    "graph.set_entry_point(\"plan\")\n",
    "graph.add_edge(\"plan\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"synthesize\")\n",
    "graph.add_edge(\"synthesize\", \"reflect\")\n",
    "\n",
    "def route_after_reflect(state: AgentState):\n",
    "    if should_retry(state):\n",
    "        return \"retrieve\"\n",
    "    if state[\"step_idx\"] + 1 < len(state[\"plan\"]):\n",
    "        state[\"step_idx\"] += 1\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"finalize\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    route_after_reflect,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"finalize\": \"finalize\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"finalize\", END)\n",
    "app = graph.compile()\n",
    "print(\"Graph compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed7f83",
   "metadata": {},
   "source": [
    "### Agent Inference\n",
    "Három kérdés feltevés a három különböző tudományos cikkből, amelyek a vektor tudástárba vannak betöltve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2db55b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:59:17.196483Z",
     "iopub.status.busy": "2025-11-12T14:59:17.196251Z",
     "iopub.status.idle": "2025-11-12T15:00:48.869906Z",
     "shell.execute_reply": "2025-11-12T15:00:48.869095Z",
     "shell.execute_reply.started": "2025-11-12T14:59:17.196465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Answer for Provide a concise summary of the core ideas of the Transformer architecture. Include citations.:\n",
      "The Transformer architecture, introduced as a novel approach to sequence transduction, comprises stacked self-attention mechanisms and point-wise, fully connected layers for both the encoder and decoder. Each layer includes a multi-head self-attention mechanism followed by a position-wise feed-forward network, both wrapped in residual connections with layer normalization. This design facilitates efficient parallel processing of sequences without the need for recurrence. According to [0], the architecture is composed of six identical layers, each containing these two sub-layers. The Transformer's design offers advantages over traditional recurrent neural networks, including faster training times and improved performance on translation tasks [1]. Various variations of the Transformer have been explored, showcasing its adaptability and effectiveness across diverse applications [2]. For example, Table 3 in [2] illustrates how different configurations of the Transformer affect its performance on the English-to-German translation task, emphasizing the balance between model complexity and computational efficiency. In summary, the Transformer architecture utilizes self-attention mechanisms for encoding and decoding, enabling efficient parallel computation and superior performance in sequence transduction tasks [0] [1] [2].\n",
      "\n",
      "Sources:\n",
      "[0] ./data/attention_is_all_you_need.pdf, page 2\n",
      "[1] ./data/attention_is_all_you_need.pdf, page 9\n",
      "[2] ./data/attention_is_all_you_need.pdf, page 8\n",
      "\n",
      "==========\n",
      "Answer for Provide a concise overview of Synthseg and why it is considered a novelty in practice. Include citations.:\n",
      "Synthseg is a novel approach to brain segmentation that trains a model exclusively on synthetic data, eliminating the need for extensive manual labeling and real-world data. This approach contrasts with traditional methods that require extensive real-world data and frequent retraining for different conditions. As highlighted in [0], \"First, it is only trained once and only requires a single set of anatomical segmentations (no real images), as opposed to supervised methods, which need paired images and labels for every new domain.\" Furthermore, [1] notes that SynthSeg achieves superior generalization compared to supervised CNNs, domain adaptation techniques, and Bayesian segmentation, demonstrating its effectiveness across multiple datasets and modalities. The paper also showcases that Synthseg can generalize well to new tasks such as cardiac MRI and CT scans, further validating its versatility. [2] underscores that Synthseg's robustness extends beyond simple comparisons; it can handle variations in contrast and resolution without requiring retraining, making it particularly valuable for clinical applications where consistency and efficiency are crucial. Overall, Synthseg represents a significant advancement in the field by streamlining the training process and enhancing model performance across diverse scenarios. [0], [1], [2]\n",
      "\n",
      "Sources:\n",
      "[0] ./data/synthseg.pdf, page 11\n",
      "[1] ./data/synthseg.pdf, page 0\n",
      "[2] ./data/synthseg.pdf, page 6\n",
      "\n",
      "==========\n",
      "Answer for Provide a concise description of BrainAgeNext, including its purpose and applications. Include citations.:\n",
      "Subtask 1: Provide a concise description of BrainAgeNext, including its purpose and applications. Include citations.\n",
      "\n",
      "BrainAgeNeXt is a sophisticated convolutional neural network specifically designed to estimate brain age from T1-weighted magnetic resonance imaging (MRI) scans. This model was developed using the MedNeXt framework and was trained on a dataset comprising 11,574 MRI scans from various sources. BrainAgeNeXt demonstrated remarkable performance with a mean absolute error of 2.78 ± 3.64 years, outperforming three leading methods in the field. The architecture of BrainAgeNeXt comprises four MedNeXt blocks followed by a regression header, integrating insights from Vision Transformers and Swin Transformers. This design enhances the model's ability to generalize across different MRI modalities, including 7T MRI scans. The model's accuracy and versatility make it a valuable tool for researchers and clinicians aiming to assess brain aging and diagnose neurodegenerative diseases. [0], [1], [2] To validate its robustness, the authors employed spatial and intensity augmentations during training, ensuring the model's reliability across varied imaging conditions. [2] In summary, BrainAgeNeXt represents a significant advancement in the field of brain age prediction, offering both clinical and research applications. Further studies are necessary to explore its full potential and clinical utility. [0], [1], [2]\n",
      "\n",
      "Sources:\n",
      "[0] ./data/brainagenext.pdf, page 0\n",
      "[1] ./data/brainagenext.pdf, page 6\n",
      "[2] ./data/brainagenext.pdf, page 19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_agent(question: str):\n",
    "    init: AgentState = {\n",
    "        \"question\": question,\n",
    "        \"plan\": [],\n",
    "        \"step_idx\": 0,\n",
    "        \"retrieved\": [],\n",
    "        \"partial_answers\": [],\n",
    "        \"citations\": [],\n",
    "        \"final_answer\": None,\n",
    "        \"reflect_score\": 0.0,\n",
    "        \"history\": []\n",
    "    }\n",
    "    out = app.invoke(init)\n",
    "    return out[\"final_answer\"], out\n",
    "\n",
    "queries = [\"Provide a concise summary of the core ideas of the Transformer architecture. Include citations.\",\n",
    "\"Provide a concise overview of Synthseg and why it is considered a novelty in practice. Include citations.\",\n",
    "\"Provide a concise description of BrainAgeNext, including its purpose and applications. Include citations.\"]\n",
    "for query in queries:\n",
    "    final_answer, state = run_agent(query)\n",
    "    print(10*\"=\")\n",
    "    print(f\"Answer for {query}:\")\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2004b",
   "metadata": {},
   "source": [
    "### RAG Query Sanity Check\n",
    "Gyors „sanity check” a RAG minőségének ellenőrzésére: Ugyanarra a 3 különböző kontextusu kérdésre kilistázza a FAISS és a rerank algoritmus (CrossEncoder). Ez által láthatjuk a relevanciát és hangolhatjuk a későbbiekben a beállításokat (TOP_K, RERANK_TOP_K, CHUNK_SIZE/OVERLAP) a végső válaszgenerálás nélkül.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bae15b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:00:48.876648Z",
     "iopub.status.busy": "2025-11-12T15:00:48.876321Z",
     "iopub.status.idle": "2025-11-12T15:00:48.967231Z",
     "shell.execute_reply": "2025-11-12T15:00:48.966489Z",
     "shell.execute_reply.started": "2025-11-12T15:00:48.876623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "For query: Provide a concise summary of the core ideas of the Transformer architecture. Include citations.\n",
      "Doc 0 | chunk_id=12\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder\n",
      "---\n",
      "\n",
      "Doc 1 | chunk_id=47\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signi\n",
      "---\n",
      "\n",
      "Doc 2 | chunk_id=40\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "---\n",
      "\n",
      "Doc 3 | chunk_id=39\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "6.2 Model Variations\n",
      "To evaluate the import\n",
      "---\n",
      "\n",
      "Doc 4 | chunk_id=10\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction\n",
      "---\n",
      "\n",
      "==========\n",
      "For query: Provide a concise overview of Synthseg and why it is considered a novelty in practice. Include citations.\n",
      "Doc 0 | chunk_id=254\n",
      "each region, slice selection proﬁle, etc.). Second, this result is\n",
      "consistent with converging evidence that augmenting the data\n",
      "beyond realism often leads to better generalisation [61, 9, 4].\n",
      "Additionally, SynthSeg enables to greatly alleviate the la-\n",
      "belling labour for training purposes. First, it \n",
      "---\n",
      "\n",
      "Doc 1 | chunk_id=175\n",
      "learn from labels obtained by automated methods on diverse populations (e.g., ageing\n",
      "and diseased), thus achieving robustness to a wide range of morphological variability.\n",
      "We demonstrate SynthSeg on 5,000 scans of six modalities (including CT) and ten reso-\n",
      "lutions, where it exhibits unparalleled ge\n",
      "---\n",
      "\n",
      "Doc 2 | chunk_id=224\n",
      "and generalisation of SynthSeg. First, we compare it against\n",
      "all competing methods on every dataset. Then, we conduct\n",
      "an ablation study on the proposed method. The third exper-\n",
      "iment validates SynthSeg in a proof-of-concept neuroimaging\n",
      "group study. Finally, we demonstrate the generalisability of ou\n",
      "---\n",
      "\n",
      "Doc 3 | chunk_id=183\n",
      "SynthSeg for brain segmentation, and contours of the corresponding\n",
      "ground truth. (b) Test-time segmentations for a variety of contrasts and\n",
      "resolutions, on subjects spanning a wide age range, some presenting large\n",
      "atrophy and white matter lesions (green arrows). All segmentations are\n",
      "obtained with t\n",
      "---\n",
      "\n",
      "Doc 4 | chunk_id=184\n",
      "can be obtained automatically (rather than manually), since the\n",
      "training scans are directly generated from their ground truths,\n",
      "and are thus perfectly aligned with them. This enables us to\n",
      "greatly improve the robustness of SynthSeg by including auto-\n",
      "mated training maps from highly diverse populatio\n",
      "---\n",
      "\n",
      "==========\n",
      "For query: Provide a concise description of BrainAgeNext, including its purpose and applications. Include citations.\n",
      "Doc 0 | chunk_id=68\n",
      "variations and is a potential prognostic biomarker in neurodegenerative conditions. This study introduces\n",
      "BrainAgeNeXt, a novel convolutional neural network inspired by the MedNeXt framework, designed to predict\n",
      "brain age from T1-weighted magnetic resonance imaging (MRI) scans. BrainAgeNeXt was trai\n",
      "---\n",
      "\n",
      "Doc 1 | chunk_id=94\n",
      "7\n",
      "Network architecture\n",
      "We propose BrainAgeNeXt, a CNN inspired by the MedNeXt framework58, which predicts brain age from\n",
      "minimally preprocessed 3D T1w MRI. BrainAgeNeXt architecture is composed of four MedNeXt blocks\n",
      "followed by a regression header. The MedNeXt blocks are a 3D extension of the ConvN\n",
      "---\n",
      "\n",
      "Doc 2 | chunk_id=145\n",
      "20\n",
      "BrainAgeNeXt demonstrated excellent performance in predicting brain age, the clinical utility of brain age as a\n",
      "prognostic biomarker for MS requires further validation through longitudinal studies with larger cohorts and\n",
      "testing in the context of interventional clinical trials. Finally, the perfo\n",
      "---\n",
      "\n",
      "Doc 3 | chunk_id=96\n",
      "generalizability of BrainAgeNeXt. The spatial transformations consisted of random rotations along the three\n",
      "axes (up to 0.1 radiant), affine transformations, zooming in and out (up to 5% of the image size), and\n",
      "foreground cropping and padding to the input dimensions (160,192,160). The intensity augm\n",
      "---\n",
      "\n",
      "Doc 4 | chunk_id=100\n",
      "compare with our proposed BrainAgeNeXt using the same training dataset. BrainAgeNeXt, inspired by the\n",
      "ConvNeXt59 and MedNeXt58 architectures, differs from the baseline DenseNet by incorporating advanced\n",
      "convolutional techniques for improved feature extraction, performance, and generalization in brai\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Provide a concise summary of the core ideas of the Transformer architecture. Include citations.\",\n",
    "\"Provide a concise overview of Synthseg and why it is considered a novelty in practice. Include citations.\",\n",
    "\"Provide a concise description of BrainAgeNext, including its purpose and applications. Include citations.\"]\n",
    "\n",
    "for query in queries:\n",
    "    print(10*\"=\")\n",
    "    print(\"For query:\", query)\n",
    "    docs = retriever.invoke(query)  \n",
    "    docs = rerank(query, docs) if ENABLE_RERANKER else docs[:5]\n",
    "    for i, d in enumerate(docs):\n",
    "        print(f\"Doc {i} | chunk_id={d.metadata.get('chunk_id')}\\n{d.page_content[:300]}\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34cbe3",
   "metadata": {},
   "source": [
    "## Rövid Dokumentáció és Értékelés\n",
    "\n",
    "### Notebook\n",
    "A notebook lépésről lépésre futtatható. Config → Adatbetöltés és szeletelés → Beágyazások és vektortár → Rerankelés → LLM betöltés → LangGraph Agent pipeline (plan → retrieve → synthesize → reflect → finalize) → Kérdések futtatása → RAG query megtekintése.\n",
    "\n",
    "\n",
    "### Technológiák\n",
    "- **PyPDFLoader**: Megbízható PDF feldolgozás, a `langchain_community` ökoszisztémába illeszkedik; a metaadatok (pl. oldal) automatikusan kezelhetők.\n",
    "- **RecursiveCharacterTextSplitter**: `CHUNK_SIZE=800`, `CHUNK_OVERLAP=120` balansz a kontextus-megőrzés és a token-költség/latencia között; stabil általános beállítás tudományos PDF-ekhez.\n",
    "- **Embedding: intfloat/e5-small-v2**: E5-széria, főleg erős zero-shot információkeresésben; a „query:”/„passage:” prefix segíti a lekérdező–passage illesztést. Kis modell → gyors CPU/MPS futás.\n",
    "- **FAISS CPU**: Gyors, lokális, könnyen fenntartható lokális tárban, általánosságban jó kiindulás kisebb korpuszon. `TOP_K=8` a visszakeresésnél.\n",
    "- **Rerank: cross-encoder/ms-marco-MiniLM-L-6-v2**: A retrieval pontosságnövelése érdekében raktam bele a pipelineba, hogy a legjobb 5 találatot újra rangsorolja. Egy külön,light wegiht modell, amely közvetlenül a (kérdés, eredmény) párokat pontozza.\n",
    "- **LLM: Qwen/Qwen2.5-3B-Instruct, transformers**: Kompetens, de még pont PC-n helyben futtatható; `device_map=\"auto\"` MPS/GPU támogatással, `temperature` és `max_new_tokens` szabályozza a stílust és hosszt.\n",
    "\n",
    "\n",
    "\n",
    "### Teljesítmény és tesztelés\n",
    "A minőségi teszteléshez érdemes egy kézzel készített aranykészletet összeállítani változatos, valós kérdés–válasz párokkal és a hozzájuk tartozó releváns source sectionökkel, mert ez ad objektív alapot tud adni a méréshez. Ezenkívül automatizált metrikákkal kell mérni a retrievel és a válaszok minőségét, például Recall@K, MRR, groundedness, factuality, relevancia, teljesség és helyes citáció, szükség esetén LLM-es bírálóval, amelyet időnként emberi ellenőrzéssel kalibrálunk. Hasznos egy egyszerű RAG-eval pipeline, amely a retrieve–rerank–generate lépések után kiszámítja a mutatókat, riportot készít és felsorolja a hibás eseteket, hogy látható legyen, hol romlik a rendszer. Érdemes külön stresszteszteket futtatni különböző kérdéstípusokra és edge case-ekre. Fontos az is hogy rosszabb eredményekhez logoljunk kérdést, találatokat, pontszámokat, promptot és választ, hogy később könnyebb legyen a hiba elkülönítése a retrieval és a generálás közül. \n",
    "\n",
    "### Javasolt további fejlesztések\n",
    "Egy fontos upgrade a rendszer továbbfejlesztéséhez lenne fennmaradásának biztosítása, ami a FAISS-index mentését és újratöltését, valamint a beágyazások cache-elését a gyorsabb kiszolgálás érdekében. Továbbá a keresés minősége hibrid megközelítéssel is talán javítható, a lexikális BM25 és sűrű vektoros módszerek kombinálásával, kiegészítve MMR-rel és multi query expansionnel a lekérdezések sokszínűségére. A rangsorolásnál mérlegelhető egy nagyobb modell használata a jobb minőségért, vagy akár teljesen kikapcsolni a nagyobb sebességért, mindkét esetben érdemes a RERANK_TOP_K paraméter finomhangolása. Az LLM inferencia gyorsítható bitkvantálással, kisebb modellek, például Qwen2.5 1.5B bevetésével, vagy clusteren való távoli inferenciával. Az agent pipeline is továbbfejleszthető webes kereséssel vagy egyéb ;POST' típusú tool-ok hozzáadásával. A kontextuskezelés javítható hosszabb passzusok kinyerésével, extractive vagy összegző megközelítéssel, így több releváns forrás férhet be a promptba. Emellett célszerű a promptot és a főbb hiperparamétereket, így a CHUNK_SIZE, a CHUNK_OVERLAP, a TOP_K, a MAX_NEW_TOKENS és a TEMPERATURE szisztematikusan grid vagy bayesi optimalizálással hangolni. Az explainability érdekében hasznos részletes metrikákat és trace-eket gyűjteni. Végül érdemes egy quality control keretet is természetesen kialakítani, amely automatizált RAG értékelési pipeline-t tartalmaz."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
